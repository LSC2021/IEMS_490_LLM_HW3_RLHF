nohup: ignoring input
Starting PPO...
Loading tokenizer...
Loading dataset: anthropic/hh-rlhf ...
Original train size: 50000
Original test size:  8552
Filtering edge cases (same as assignment3/data_preprocessing)...
Filtered train size: 44379
Building PPO prompts from 'chosen' conversations...
Number of PPO prompts: 44379
Loading policy and reference models...
Loading reward model from ./final_reward_model ...
PPO config: PPOConfig(total_steps=1500, rollout_batch_size=8, ppo_epochs=2, gradient_accumulation_steps=1, clip_range=0.1, kl_coef=0.05, target_kl=0.3, adapt_kl=True, entropy_coef=0.1, target_entropy=1.8, value_coef=0.3, learning_rate=5e-06, weight_decay=0.01, max_grad_norm=1.0, gamma=1.0, lam=0.95, max_prompt_length=256, max_new_tokens=64, temperature=1.0, top_p=0.9, top_k=0, save_every=100, log_every=1, fp16=True)
PPO training complete. Final model saved to ./ppo_rlhf_model
PPO finished. Starting GRPO...
Loading tokenizer...
Loading dataset: anthropic/hh-rlhf ...
Original train size: 50000
Original test size:  8552
Filtering edge cases (same as data_preprocessing.py)...
Filtered train size: 44379
Building GRPO prompts from 'chosen' conversations...
Number of GRPO prompts: 44379
Loading GRPO policy and reference models...
Loading reward model from ./final_reward_model ...
GRPO config: GRPOConfig(total_steps=1500, rollout_batch_size=2, group_size=3, ppo_epochs=1, gradient_accumulation_steps=1, kl_coef=0.02, target_kl=0.3, adapt_kl=False, entropy_coef=0.1, target_entropy=1.8, learning_rate=2e-06, weight_decay=0.01, max_grad_norm=1.0, gamma=1.0, lam=0.95, max_prompt_length=192, max_new_tokens=32, temperature=1.0, top_p=0.9, top_k=0, save_every=100, log_every=1, fp16=True)
GRPO training complete. Final model saved to ./grpo_rlhf_model
Best-by-reward checkpoint saved to ./grpo_best
GRPO finished. Starting DPO...
Loading dataset for DPO...
DPO train triples: 140355
Total training steps (optimizer updates): 4387
Using device: cuda
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_500
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_1000
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_1500
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_2000
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_2500
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_3000
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_3500
Saved checkpoint to ./dpo_rlhf_model/checkpoint_step_4000
Training complete. Saving final DPO policy to ./dpo_rlhf_model...
Done.
All training finished.
